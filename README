
public class DeepLearning {
	public static void main(String[] args) {

/**
 * 
 * １．第一章　ニューラルネットワーク
 * 	 １．単層ニューラルネットワーク
 * 			1. パーセプトロン（ステップ関数）
 * 			2. ロジスティック回帰（シグモイド関数）
 * 			3. 多クラスロジスティック回帰（ソフトマックス関数）
 * 
 *	 ２．多層ニューラルネットワーク
 *			4. 多層パーセプトロン（多層ニューラルネットワーク） 
 *			   入出力層(ロジスティック回帰モデル)/隠れ層
 *
 *
 * ２．第二章　ディープラーニング
 *   １．ディープビリーフネット
 *     	 	プレトレーニング (制約つきボルツマンマシン）
 *   	 	ファインチューニング (ロジスティック回帰モデル）
 *   
 *   ２．デノイジング・オートエンコーダ
 *   	 	プレトレーニング（デノイジング・オートエンコーダ）
 *   	 	ファインチューニング(ロジスティック回帰モデル）
 *   
 *   ３．積層デノイジング・オートエンコーダ
 *   	 	プレトレーニング（積層デノイジング・オートエンコーダ）
 *   	 	ファインチューニング(ロジスティック回帰モデル）
 *   
 *   ４．ドロップアウト (ReLU ソフトプラス関数)
 *   
 *   ５．畳み込みニューラルネットワーク（高次元ニューラルネットワーク）
 */
/**
  * ****************************************************
  * Javaで学ぶニューラルネットワークとディープラーニング
  * 一般社団法人 共同通信社　suzuki.iichiro@kyodonews.jp
  * 
  * １．ニューラルネットワーク ←
  * ２．ディープニューラルネットワーク
  * 
  * ****************************************************
  * 
  * ディープラーニングはニューラルネットワークを応用したもの。
  * ニューラルネットワークは人間の脳をまねた機械学習アルゴリズム。
  * ディープラーニングとは、「機械自らが与えられたデータの中から、特徴量を見つ
  * け出し、学習する」事。
  * 
  * １９５０年代後半第一次人工知能ブーム
  * 人間が定義した決められたルールに基づいた探索プログラム
  *   深さ優先探索・幅優先探索
  * 膨大な数の勝ち負けのパターン、過去の戦いのデータ、駒の許容異動は二などをあらか
  * じめ機械に入力しておく。機械は駒の配置を読み取り、膨大な数のパターンの中から次
  * のベストな一手を決めることができる。チェスや将棋などでは一定の成果を上げること
  * ができた。
  * 
  * できないこと
  * 無数のパターンの中から、関係のない選択肢を切り捨てること。
  * →フレーム問題
  * 
  * １９８０年代第二次人工知能ブーム
  * もし機械に世界のあらゆる知識が集約され、機械がその知識を理解すれば、どんな複雑
  * な問題が与えられても機械が正しい答えを導き出してくれるはずだ。 あらかじめ定義さ
  * れた辞書を機械に入力しておけば、機械がその文章と辞書を照らし合わせ、人間の問い
  * に機械が反応し答えることができる。
  * 
  * できないこと
  * 現実世界の知識全てを定義することはできない。 機械は知っている問題に対しては素早
  * く対応できたが、知らない問題に出会うとても足も出なかった。
  *   →推論の限界 機械は本当の意味を理解しているわけではない
  * 　→シンボルグラウンディング
  * 
  * ２０００年第三次人工知能ブーム
  * 確率・統計モデルに基づくパターン認識と分類
  * 機械の高い計算力及び高速処理能力を武器に大量のトレーニングデータを使い複雑な問
  * 題をYes/Noに置き換えた上で、どんなデータがYesなのか、またどんなデータがNoなのか
  * について規則性（パターン）を見つけ出し学習し分類した。
  * 
  * できないこと
  * 機械学習のパターンモデルパラメータといった特徴量設計。
  * 人間の勘やセンスにたより。膨大で細かいパラメータ調整に明け暮れる。
  * 
  * ・フレーム問題
  * ある課題が機械に与えられたときに、どの知識を使うべきか機械が認識できない。
  * 
  * ・シンボルグラウンディング
  * 機械が知識を記号としてしか認識していないために、知識を組み合わせた概念を機械が
  * 理解できない。
  * 
  * ・特徴量設計
  * 機械が対象物の何が特徴なのか判らない。
  * 
  * →　機械が「ある事象のどの特徴に注目して、どのような情報を使えばよいのか」がわか
  * れば解決できる。
  * 
  * →→機械が与えられたデータから特徴を抽出し、
  *   その特徴に基づいたどの知識を使うべきかを判断し(フレーム問題解決）
  *   その知識を組み合わせた概念を理解し(シンボルグラウンディング解決）
  *   状況や目的に合わせて適切な知識を自ら取得し解決できる。
  * 
  * ２００６年第四次人工知能ブーム
  * 
  *   ディープラーニング
  *   機械が与えられたデータから何が重要な特徴量であるかを見つけ出す手法
  * 
  *   プレトレーニング
  * 　ニューラルネットワーク時代　
  * 　学習時に発生した誤差をネットワーク全体にフィードバックする。層が積み重なった
  * ネットワークでは、誤差が消えてしまう。幾重にも積み重ねられた層のネットワーク全
  * 体を一つの巨大なニューラルネットワークとして、一度に学習させようとしたため。
  * 
  * 　ディープラーニング時代
  *   次元の低い層の特徴を学習し、次の層の入力データとして扱う。おおよその全体像を
  *   つかんでから細部の特徴を次の層で学習する。
  * 
  *   ドロップアウト
  * 　ネットワーク内のいくつかのつながりをランダムに切断し学習する。これにより学習
  * のステップごとに違うネットワークが形成され、ネットワークの重みの調整がうまくい
  * く。
  * 
  * できないこと
  * 学習が完了するまでのステップが長い。途方もないリソースが必要。
  * Googleの猫　１０００台で３日
  * 
  * =====================================================================
  * 機械学習による分離
  * 
  * ２種類のデータには、二つを分ける境界線がありそうだ。
  * 境界線をどこに奥かを決めればよい。
  * では早速境界線を決めよう。→　実際には明確に境界線を定義することは難しい
  * 
  * 線形分離　データ群を直線で分離することができる場合
  * 非線形分離　データ群を直線では分離できない場合
  * 
  * 機械学習により決定される境界を識別境界といい、直線か非直線をか問わない
  * 
  *
  *  	単層ニューラルネットワークの問題点
  *  		非線形問題を解くことができない
  *  
  *  	入力層と出力層の間に隠れ層を加えた、多層ニューラルネットワークで解決できる。
  *  	なぜ、多層ニューラルネットワークが非線形問題を解くことができるのか。
  * 			→　できる
  *  
  *  	１．ユニットを増やすことでより複雑な関数を揚言する能力を持てるようになったから
  *  	２．モデルが非線形問題も解くことができるようになるための鍵は、
  * 	 		バックプロパゲーションアルゴリズム。
  *  		出力の誤差をネットワーク全体に逆伝播することで、
  *  		各反復でモデルがトレーニングデータに適合するよう更新され、
  *  		最終的にデータの関数に近似するよう最適化される。
  * 
  */

		/**
		 * １．ニューラルネットワーク 
		 * 
		 * パーセプトロン ステップ関数 
		 * パーセプトロンは、ニューラルネットワークのアルゴリズムの中でも
		 * 最も単純な作りをしたモデルであり、二つのクラスを線形識別できる 
		 * モデルでニューラルネットワークの原型でもある。
		 * データセットが線形に分けられることが保証されている場合、アルゴリズムの精
		 * 度は学習率の値とは無関係であるため、学習率の値は１でよい。
		 */
//		Perceptrons pt = new Perceptrons(2);
//		pt.setDataSet(); // データセット
//		pt.buildModel(); // モデルの構築
//		pt.evaluetion(); // 評価テスト
		/**
		 * ロジスティック回帰 シグモイド関数
		 * 連続値を取る二つの変数からなる式を想定した回帰モデルで、
		 * パーセプトロンを一般化した線形分離モデルで、ニューラルネットワーク の一
		 * つである。
		 * パーセプトロンでは、活性化関数にステップ関数を用いているが、 ロジスティッ
		 * ク回帰では、シグモイド関数を用いている。
			 * シグモイド関数は、任意の実数を０から１の値に写像する。
		 * つまりロジ スティック回帰の出力は、それぞれのクラスの事後確率と見なすこ
		 * とができる。
		 */
//		LogisticRegression lr = new LogisticRegression(2, 3);
//		lr.setDataSet();
//		lr.buildLogisticRegressionModel();
//		lr.evaluate();
//		lr.print();
		/**
		 * 多クラスロジスティック回帰 ソフトマックス関数
		 * ロジスティック回帰は多クラス分類にも応用することができる。
		 * 2クラス分類では、活性化関数がシグモイド関数であり、出力が０から１になる
		 * ことで、データがどちらのクラスに所属する確率が高いかを求めることができた。
		 * クラス数がＫのときは、各クラスの所属度を表す確立ベクトル（出力がＫ次元）と 
		 * することで多クラスデータも分類できる。
		 * シグモイド関数の多変量版であるソフトマックス関数を用いる。
		 */
//		LogisticRegressionXOR lrXOR = new LogisticRegressionXOR(2, 3);
//		lrXOR.setDataSet();
//		lrXOR.buildLogisticRegressionModel();
//		lrXOR.print();
		/**
		 * 多層パーセプトロン（多層ニューラルネットワーク） 
		 * 入出力(ロジスティック回帰モデル）と隠れ層
		 * 
		 * 単層ニューラルネットワークには大きな問題があった。
		 * 線形分離できる問題には効率的なパーセプトロンやロジスティック回帰であるが、
		 * 非線形問題を全く解くことができないのである。
		 * 最も単純なXOR（排他的論理和問題）でさえとくことができない。
		 * 現実問題のほとんどは非線形であるため、パーセプトロンやロジスティック回帰
		 * では実用性がない。
		 * そこで非線形の問題にも対応できるようにアルゴリズムを改良したものが、
		 * 多層ニューラスネットワークである。
		 * 入力層と出力層の間に「隠れ層」と呼ばれる層を加えることで、
		 * ネットワークが様々なパターンを表現できるようにした。
		 * 個々で重要なのは、隠れ層を飛び越えた入力層と出力層の直結を導入せず、
		 * 必ず、フィードフォワードネットワークの構造にしておくこと。
		 * こうすることで、隠れ層の数が多くなろうとも、数学的に複雑になりすぎずに、
		 * 任意の関数を近似することができる。
		 * 隠れ層にはHiddenLayerクラス、出力層にはLogisticRegressionクラスが定義されている。
		 * 出力の誤差をネットワーク全体に逆伝播することにより、各反復でモデルがトレー
		 * ニングデータに適合するよう更新されるバックプロパゲーションが鍵となる。
		 */
//		MultiLayerPerceptrons mlpt = new MultiLayerPerceptrons(2, 3, 2, new Random(123));
//		mlpt.setDataSet();
//		mlpt.buildMultiLayerPerceptronsModel();
//		mlpt.evaluate();
//		mlpt.print();
		
/**
  * ****************************************************
  * Javaで学ぶニューラルネットワークとディープラーニング
  * 一般社団法人 共同通信社　suzuki.iichiro@kyodonews.jp
  * 
  * １．ニューラルネットワーク
  * ２．ディープニューラルネットワーク ←
  * 
  * ****************************************************
  * 
  *  前項からのあらすじ
  *  パーセプトロンでは解決できなかった非線形分類も、多層ニューラルネットワークで
  *  すなわち、入力層と出力層の間に隠れ層を追加することで、学習することができた。
  *  
  *  理由
  *  ニューロンの数が増えることで、ニューラルネットワーク全体で表現できるパターンが
  *  増えたから。
  *  
  *  では、隠れ層の数をもっと増やせば。隠れ層をもっと積み重ねれば、どんなに複雑な
  *  問題でも解決できるのではないか。
  *  →この試みはうまくいかなかった。
  *  単純に層を積み重ねるだけでは逆に、層の数が少ないものより予測の精度が下がってしまう。
  *  
  *  なぜ。
  *  フィードフォワードネットワークが持つ特徴に原因があった。
  *  多層ニューラルネットワークで効率的に学習誤差をネットワーク全体に伝播するには、
  *  バックプロパゲーションアルゴリズムが用いられる。このアルゴリズムでは、
  *  誤差はニューラルネットワークの各層を逆順に、一層ずつ順番に入力層まで辿る。
  *  出力層での誤差を入力層まで逆伝播することで、隠そうごとに順番にネットワークの重みが
  *  調整され、ネットワーク全体の重みが最適化される。層の数が多くなったときには、
  *  層をさかのぼるごとに誤差が次第に消えて行ってしまい、ネットワークの重みを調整できない。
  *  入力層に近いそうでは、誤差が全くフィードバックされなくなってしまったのだ。
  *  
  *  層と層の間が密につながったニューラルネットワークには、重みを調整する機能が欠如して
  *  しまっていた。この問題を、勾配消失問題という。
  *  
  *  ディープラーニングではこの問題は起きない。なぜか。
  *  
  *  １．ディープビリーフネット
  *  ２．積層デノイジング・オートエンコーダ
  *  
  *  上記二つのアルゴリズムによって、深層にもかかわらず高い予測精度を記録した。
  *  この二つのアルゴリズムに共通する、勾配消失問題を解決した手法とは何か。
  *  
  *  レイヤーワイズトレーニング   各層ごとに学習をするという手法。
  *  これにより、各層がそれぞれ独立してネットワークの重みを調整するため、
  *  層が幾重に積み重なったも、ネットワーク全体（すなわちモデルのパラメータ）が適切に最適化
  *  されるようになった。
  *  
  *  ニューラルネットワークでは、ユニット数や層の数が多いほど、理論上は表現力が増え、
  *  解くことができる問題も増える。それがうまくいかなかったのは、誤差を各層に正しくフィードバックできず、
  *  ネットワーク全体としてのパラメータを適切に調整できなかったからだ。
  *  
  *  ディープラーニングでは、レイヤーワイズトレーニングにより、
  *  各層がそれぞれ独立してネットワークの重みを調整するため、層が幾重に積み重なっても、
  *  ネットワーク全体（すなわちモデルのパラメータ）が適切に最適化されるようになった。
  *  
  *  「出力層で発生した誤差を入力層まで行きに逆伝播して学習しようとしたらうまくいかなかったので、
  *  各層ごとに分けて学習を行い、誤差を層ごとにすぐに反映させることでうまくいくようになった」
  *  
  *  レイヤーワイズトレーニングのテクニック
  *  ・プレトレーニング（事前学習 各層ごとの学習）
  *  ・ファインチューニング（微調整 ネットワーク全体に対する調整）
  *  
  *  「前後の層がともに隠れそうでは、入力層も出力層もないが、どのように学習すればよいのだろう。
  *  何が入力になり、そしてどのように出力すればよいのだろう」
  *  
  *  ニューラルネットワークでは、入力層と隠れ層、隠れ層と出力層の間で、ネットワークパラメータ
  *  重みとバイアスを調整する必要があった。
  *  ディープラーニングでは、加えて、隠れ層と隠れ層の間でも重みとバイヤスの調整が必要である。
  *  
  *  入力について
  *  	前の層から伝播してきた値がそのまま入力になる。
  *  	前の層から伝播してきた値とは、ネットワークの重みを用いて前の層から現在の層まで順伝播してきた値。
  *  	→	一つ以上前の層が学習した特徴が現在の層の入力となり、
  *  		現在の層は底から亜新たに与えられたデータの特徴を学び取る
  *  		→　入力データから段階的に特徴を学び取っていく
  *  			→　層がより深くなるほど、より高次な特徴を学習する
  * 	「機械が概念を学び取れるようになった」
  * 
  * 出力について
  * 	ディープビリーフネットと積層デノイジング・オートエンコーダともに、次の条件を満たすように学習をする。
  * 	「出力値と入力値と同じになるように学習する」
  * 	→　入力層から入ってきた値は、隠れ層を通ってまた入力層に戻る。
  * 		その時の誤差がなくなるように（すなわち出力値と入力値が一致するように）ネットワークの重みを調整する。
  *			 
  * ファインチューニング
  * 	役割
  * 
  * 	1. 出力層手前までの各層に対してプレトレーニングが終了したら、
  * 	出力層を加えたニューラルネットワーク全体に対してトレーニングデータで教師有り学習を行う。
  * 	「出力層における教師有り学習には、
  * 		計算量と得られる精度とのバランスからロジスティック回帰を用いることが多い。」
  * 	2. ディープニューラルネットワーク全体の重みの最終調整を行う。
  *		「すなわちバックプロパゲーションアルゴリズム」が適用される。	
  * 
  * 	勾配消失問題は発生しないのか？
  *		プレトレーニングを挟んだ場合、ネットワークの重みはプレトレーニングによってほぼ調整された状態になり、
  *		その後で学習が行われる。そのため適切な誤差が入力層に近い層まできちんと伝播する。
  * 
  *	ディープラーニング
  *		プレトレーニングでは、モデルのパラメータは層ごとに最適化され、
  *		ファインチューニングで一つのディープニューラルネットワークとして調整される。
  *
  * 
  * ディープラーニングのアルゴリズム
  *  １．ディープビリーフネット
  *  	プレトレーニング (制約つきボルツマンマシン）
  *  	ファインチューニング 
  *  ２．デノイジング・オートエンコーダ
  *  	プレトレーニング（デノイジング・オートエンコーダ）
  *  	ファインチューニング
  *  ３．積層デノイジング・オートエンコーダ
  *  	プレトレーニング（積層デノイジング・オートエンコーダ）
  *  	ファインチューニング
  *  ４．ドロップアウト (ReLU ソフトプラス関数)
  *  ５．畳み込みニューラルネットワーク（高次元ニューラルネットワーク）
  *  	畳み込み
  *  	プーリング
  *  
  */
		/**
		 * 		ディープビリーフネット
		 *		プレトレーニング（制約つきボルツマンマシン）
		 */
//        RestrictedBoltzmannMachines RBM = new RestrictedBoltzmannMachines();
//        RBM.setDataSet();
//        RBM.buildBoltzmannMachinesModel();
//        RBM.evaluate();
		/**
		 * 		ディープビリーフネット
		 * 		ファインチューニング（ロジスティック回帰モデル）
		 * 			デモデータでは、入力層のユニット数は60、隠れ層は2層。
		 * 			どちらの隠れ層もユニット数は20，出力層のユニット数は3になっている。
		 * 
		 *  制約つきボルツマンマシン
		 *  他の全てのユニットと結合したネットワーク
		 */
//		DeepBeliefNets DBN = new DeepBeliefNets();
//        DBN.setDataSet();
//        DBN.buildDeepBeleifNetsModel();
//        DBN.evaluate();
        /**
         * 	
		 *  	デノイジング・オートエンコーダ
		 *  		プレトレーニング（デノイジング・オートエンコーダ）
		 * 			ファインチューニング（ロジスティック回帰モデル）
		 * 
		 * 		デノイジング・オートエンコーダ
		 *  		プレトレーニングの特徴である出力値が入力値と同じになるように学習するという部分がより強調された手法
		 *  	
		 *  	「入力データに意図的にノイズを加えてデータの一部を損傷させてしまう。
		 *  	その損傷したデータを元の入力データに復元するように、デノイジング・オートエンコーダは学習する。」
         */
//		DenoisingAutoencoders DA = new DenoisingAutoencoders();
//		DA.setDataSet();
//		DA.buildDenoisingAutoencodersModel();
//		DA.evaluate();
		/**
		 *  	積層デノイジング・オートエンコーダ
		 *  		プレトレーニング（積層デノイジング・オートエンコーダ）
		 * 			ファインチューニング（ロジスティック回帰モデル）
		 * 
		 * 		※積層デノイジング・オートエンコーダ
		 * 			デノイジング・オートエンコーダの層を積み重ねたディープニューラルネットワーク
		 *  	
		 */
//		StackedDenoisingAutoencoders SDA = new StackedDenoisingAutoencoders();
//		SDA.setDataSet();
//		SDA.buildStackedDenoisingAutoencodersModel();
//		SDA.evaluate();
		/**
		 * ドロップアウト
		 *  
		 * 単純に多くの層を積み重ねたニューラルネットークでは、途中まで補足できていた出力誤差が
		 * 徐々に消えて行ってしまうという問題（勾配消失問題）が生じるため良い精度が出なかった。
		 * 
		 * この問題を防ぐため、プレトレーニングが必要
		 * 
		 * プレトレーニングにおける、レイヤーワイズトレーニングが問題を解決した。
		 * 
		 * レイヤーワイズトレーニング
		 * 		ディープビリーフネット
		 * 		積層デノイジング・オートエンコーダ
		 * 
		 * → プレトレーニングなしのディープラーニングアルゴリズムによって、
		 * 		高い精度や正確性を得ることはできないのか
		 * 
		 * 「ネットワークが密につながってしまっていることが問題ならば、
		 * 		無理矢理「疎」にしてしまえばよい。
		 * 			そうすれば勾配消失問題も起こらず適切に学習できるはずだ。」
		 * 
		 *	デノイジングオートエンコーダ
		 *		入力層のユニットのみに損失を加えて学習が行われる
		 *		同じ損失データモデルが、学習で一貫して使われる
		 *	ドロップアウト
		 *		隠れ層のユニットにドロップアウトマスクをかける 
		 *		各学習の反復ごとに別のドロップアウトマスクが適用させる
		 *
		 *	ドロップアウトマスクは、学習ループごとに異なった形のニューラルネットワークで学習が行われる。
		 *	ReLU （ソフトプラス関数）
		 */
//		Dropout DO = new Dropout();
//		DO.setDataSet();
//		DO.buildDropoutModel();
//		DO.evaluate();
		/**
		 * 畳み込みニューラルネットワーク
		 * 現実世界を見ると、対象となるデータがいつも一次元であるとは限らない。
		 * その最も典型的なケースは画像データ。　→畳み込みニューラルネットワーク 
		 * 
		 * 
		 * 
		 */
//		ConvolutionalNeuralNetworks CNN = new ConvolutionalNeuralNetworks();
//		CNN.setDataSet();
//		CNN.buildConvolutionalNeuralNetworksModel();
//		CNN.evaluate();
	}
}
